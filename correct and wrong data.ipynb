{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files containing '0_' in their name: 55\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_csv_files_with_prefix(directory):\n",
    "    count = 0\n",
    "\n",
    "    # Iterate over all files and directories in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Check if \"Lshin\" directory exists\n",
    "        if 'Lshin' in dirs:\n",
    "            lshin_dir = os.path.join(root, 'Lshin')\n",
    "            \n",
    "            # Iterate over CSV files inside \"Lshin\" directory\n",
    "            for file in os.listdir(lshin_dir):\n",
    "                if file.endswith('.csv') and '0_' in file and 'SQT' in file:\n",
    "                    count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "# Specify the path where you want to search\n",
    "search_path = 'E:\\MY-GRAD\\CNN-LSTM\\physical therapy dataset\\PHYTMO\\inertial\\lower'\n",
    "\n",
    "# Call the function to count the CSV files\n",
    "csv_count = count_csv_files_with_prefix(search_path)\n",
    "\n",
    "print(f\"Number of CSV files containing '0_' in their name: {csv_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CSV files copied to correct_data.csv with '0' label.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def process_csv_files(directory, output_file):\n",
    "    with open(output_file, mode='w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Time (s)'\t,'Gyroscope X (deg/s)',\t'Gyroscope Y (deg/s)',\t'Gyroscope Z (deg/s)',\t'Accelerometer X (g)'\t,\n",
    "                         'Accelerometer Y (g)',\t'Accelerometer Z (g)','Magnetometer X (uT)'\t, 'Magnetometer Y (uT)',\t'Magnetometer Z (uT)', 'label'])  # Write header to the output file\n",
    "\n",
    "        # Iterate over all files and directories in the given directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Check if \"Lshin\" directory exists\n",
    "            if 'Lshin' in dirs:\n",
    "                lshin_dir = os.path.join(root, 'Lshin')\n",
    "                \n",
    "                # Iterate over CSV files inside \"Lshin\" directory\n",
    "                for file in os.listdir(lshin_dir):\n",
    "                    if file.endswith('.csv') and '0_' in file and 'SQT' in file:\n",
    "                        csv_file = os.path.join(lshin_dir, file)\n",
    "                        \n",
    "                        # Open the CSV file and copy its data to the output file\n",
    "                        with open(csv_file, mode='r') as infile:\n",
    "                            reader = csv.reader(infile)\n",
    "                            next(reader)  # Skip header\n",
    "                            \n",
    "                            # Write each row to the output file with '0' label\n",
    "                            for row in reader:\n",
    "                                row.append('0')  # Add '0' as the label\n",
    "                                writer.writerow(row)\n",
    "                    elif file.endswith('.csv') and '1_' in file and 'SQT' in file:\n",
    "                        csv_file = os.path.join(lshin_dir, file)\n",
    "                        \n",
    "                        # Open the CSV file and copy its data to the output file\n",
    "                        with open(csv_file, mode='r') as infile:\n",
    "                            reader = csv.reader(infile)\n",
    "                            next(reader)  # Skip header\n",
    "                            \n",
    "                            # Write each row to the output file with '0' label\n",
    "                            for row in reader:\n",
    "                                row.append('1')  # Add '0' as the label\n",
    "                                writer.writerow(row)\n",
    "\n",
    "# Specify the path where you want to search\n",
    "search_path = 'E:\\MY-GRAD\\CNN-LSTM\\physical therapy dataset\\PHYTMO\\inertial\\lower'\n",
    "\n",
    "# Specify the output file name\n",
    "output_file = 'correct_data.csv'\n",
    "\n",
    "# Call the function to process the CSV files\n",
    "process_csv_files(search_path, output_file)\n",
    "\n",
    "print(f\"Data from CSV files copied to {output_file} with '0' label.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the training data\n",
    "data = pd.read_csv('correct_data.csv')\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.drop([ 'Time (s)','Magnetometer X (uT)'\t, 'Magnetometer Y (uT)',\t'Magnetometer Z (uT)','label'], axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "classifier = SVC()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7717852874615404\n",
      "Precision: 0.7824357179694983\n",
      "Recall: 0.7780029791702279\n",
      "F1 Score: 0.7802130525284683\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Load the data\n",
    "data = pd.read_csv('correct_data.csv')\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.drop([ 'Time (s)','Magnetometer X (uT)'\t, 'Magnetometer Y (uT)',\t'Magnetometer Z (uT)' ,'label'], axis=1)\n",
    "y = data['label']\n",
    "# print(f\"Original class distribution: {y_train.value_counts()}\")\n",
    "# oversampler = RandomOverSampler()\n",
    "# X, y = oversampler.fit_resample(X, y)\n",
    "# print(f\"Original class distribution: {y_train.value_counts()}\")\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(classifier, 'random_forest_model.h5')\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load('random_forest_model.h5')\n",
    "from statistics import mode\n",
    "# Use the loaded model for predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "print(mode(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files 'Crouching2_GYRO.txt' and 'Crouching2_ACC.txt' have been concatenated into 'concatenated1.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def normalize_values(values):\n",
    "    max_val = np.max(np.abs(values))\n",
    "    if max_val != 0:\n",
    "        return values / max_val\n",
    "    return values\n",
    "\n",
    "def concatenate_files(file1, file2, output_file):\n",
    "    headers = ['Gyroscope X (deg/s)',\t'Gyroscope Y (deg/s)',\t'Gyroscope Z (deg/s)',\t'Accelerometer X (g)'\t,\n",
    "                         'Accelerometer Y (g)',\t'Accelerometer Z (g)']\n",
    "\n",
    "    with open(file1, mode='r') as file1_handle, open(file2, mode='r') as file2_handle, \\\n",
    "            open(output_file, mode='w', newline='') as output_handle:\n",
    "        reader1 = csv.reader(file1_handle)\n",
    "        reader2 = csv.reader(file2_handle)\n",
    "        writer = csv.writer(output_handle)\n",
    "\n",
    "        # Write the header to the output file\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        # Read and write the normalized records from both files\n",
    "        for row1, row2 in zip(reader1, reader2):\n",
    "            # Remove the last two elements from each row\n",
    "            row1 = row1[:-2]\n",
    "            row2 = row2[:-2]\n",
    "\n",
    "            # Convert the row values to float and normalize\n",
    "            normalized_row1 = normalize_values(np.array(row1, dtype=float))\n",
    "            normalized_row2 = normalize_values(np.array(row2, dtype=float))\n",
    "\n",
    "            # Write the concatenated normalized rows to the output file\n",
    "            writer.writerow(np.concatenate((normalized_row1, normalized_row2)))\n",
    "\n",
    "\n",
    "# Specify the paths of the input files\n",
    "file2_path = 'Crouching2_ACC.txt'\n",
    "file1_path = 'Crouching2_GYRO.txt'\n",
    "\n",
    "# Specify the path of the output file\n",
    "output_file_path = 'concatenated1.csv'\n",
    "\n",
    "# Call the function to concatenate the files\n",
    "concatenate_files(file1_path, file2_path, output_file_path)\n",
    "\n",
    "print(f\"The files '{file1_path}' and '{file2_path}' have been concatenated into '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mode\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"concatenated2.csv\"\n",
    "X_test = pd.read_csv(path)\n",
    "\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "label_map = {\n",
    "\n",
    "    0: 'Correct',\n",
    "    1: 'Incorredct',\n",
    "}\n",
    "\n",
    "print( y_pred)\n",
    "print(label_map[mode( y_pred)])\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "# conf_mat_round = np.round(conf_matrix.astype(\n",
    "#     'float')/conf_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "# # print(conf_mat_round)\n",
    "# sns.heatmap(conf_mat_round, xticklabels=label_map.values(),\n",
    "#             yticklabels=label_map.values(), annot=True, cmap='Blues', fmt=\"0.2f\")\n",
    "# # plt.title('CNN_GRU model')\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.h5']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(classifier, 'random_forest_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load('random_forest_model.h5')\n",
    "\n",
    "# Use the loaded model for predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "print(mode(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html class=\"no-js\" lang=\"\" dir=\"ltr\" id=\"htmlTag\">\n",
      "\n",
      "<head>\n",
      "  <meta charset=\"utf-8\">\n",
      "  <title>Tazkarti</title>\n",
      "  <base href=\"/\">\n",
      "\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "  <link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico\">\n",
      "  <meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\">\n",
      "  <meta http-equiv=\"Pragma\" content=\"no-cache\">\n",
      "  <meta http-equiv=\"Expires\" content=\"0\">\n",
      "\n",
      "<link rel=\"stylesheet\" href=\"styles.327c910cc4259b30e35b.css\"></head>\n",
      "\n",
      "<body>\n",
      "\n",
      "  <!-- Google Analytics here -->\n",
      "  <script>\n",
      "    (function (i, s, o, g, r, a, m) {\n",
      "      i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {\n",
      "        (i[r].q = i[r].q || []).push(arguments)\n",
      "      }, i[r].l = 1 * new Date(); a = s.createElement(o),\n",
      "        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)\n",
      "    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');\n",
      "\n",
      "    ga('create', 'UA-138651281-1', 'auto');\n",
      "  </script>\n",
      "  <!-- End Google Analytics -->\n",
      "\n",
      "  <!--<link rel=\"stylesheet\" id=\"lang\" href=\"assets/styles/styles.css\">-->\n",
      "  <app-root></app-root>\n",
      "<script type=\"text/javascript\" src=\"runtime.5c7af8918eeb4fe4c0a4.js\"></script><script type=\"text/javascript\" src=\"es2015-polyfills.f70ede54a5010ad33b36.js\" nomodule></script><script type=\"text/javascript\" src=\"polyfills.97a01cd49377eeaac7a6.js\"></script><script type=\"text/javascript\" src=\"scripts.6d9a83bfa1c904fe043c.js\"></script><script type=\"text/javascript\" src=\"main.c1f1d0ea0456bbec1b3e.js\"></script></body>\n",
      "\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "import requests\n",
    "url = \"https://tazkarti.com/#/matches\"\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
